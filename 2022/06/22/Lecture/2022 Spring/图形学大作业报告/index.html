<!DOCTYPE html><html lang="en-US" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>图形学开源报告 | 求道之人，不问寒暑</title><meta name="keywords" content="科研,2022夏季"><meta name="author" content="Eren Zhao"><meta name="copyright" content="Eren Zhao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="图形学大作业的报告...">
<meta property="og:type" content="article">
<meta property="og:title" content="图形学开源报告">
<meta property="og:url" content="http://example.com/2022/06/22/Lecture/2022%20Spring/%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%A4%A7%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/index.html">
<meta property="og:site_name" content="求道之人，不问寒暑">
<meta property="og:description" content="图形学大作业的报告...">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://pic.imgdb.cn/item/61eccb5a2ab3f51d91d621f8.jpg">
<meta property="article:published_time" content="2022-06-22T11:40:15.804Z">
<meta property="article:modified_time" content="2022-06-23T15:40:26.533Z">
<meta property="article:author" content="Eren Zhao">
<meta property="article:tag" content="科研">
<meta property="article:tag" content="2022夏季">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic.imgdb.cn/item/61eccb5a2ab3f51d91d621f8.jpg"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="http://example.com/2022/06/22/Lecture/2022%20Spring/%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%A4%A7%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '图形学开源报告',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-06-23 23:40:26'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.0.0"><style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}</style></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpeg" onerror="onerror=null;src='/img/friend.jpg'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">179</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">56</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">12</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://pic.imgdb.cn/item/61eccb5a2ab3f51d91d621f8.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">求道之人，不问寒暑</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">图形学开源报告</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-06-22T11:40:15.804Z" title="Created 2022-06-22 19:40:15">2022-06-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2022-06-23T15:40:26.533Z" title="Updated 2022-06-23 23:40:26">2022-06-23</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AF%BE%E7%A8%8B/">课程</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">6.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>22min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="图形学开源报告"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="引言">引言</h1>
<p>近年来, 随着计算能力的提高, 数据量的积累以及对动物神经网络的深入研究，人工智能领域发展迅速，尤其在机器学习方面最为突出。依据数据集是否有标记，机器学习任务被分为有监督学习、无监督学习和半监督学习。目前深度学习方法在有监督学习任务中取得令人振奋的成绩，如图像识别、语音合成、机器翻译等。</p>
<p>有监督学习依赖带标记的数据，然而大量带标记数据的获取代价昂贵，在数据生成、策略学习等学习任务中，这些标记数据的获取甚至不可行。无监督学习更符合智能的思想，研究者们普遍认为，无监督学习将会是人工智能末来重要的发展方向之一。</p>
<p>生成模型是无监督学习任务中的关键技术, 早期的生成模型有深度信念网络 (<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Deep_belief_network">deep belief network, DBN</a>) 、深度玻尔兹曼机 (deep Boltzmann machines, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine">Restricted Boltzmann Machin, RBM</a>) 、自编码器 ( <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Autoencoder">autoencoder, AE</a> )等，形成了效果出色的生成式模型，但泛化能力却不强。</p>
<p>生成对抗网络 (<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Generative_adversarial_network">generative adversarial networks, GAN</a>) 是 Goodfellow 等人在 2014 年提出的一种新的生成式模型。GAN 独特的对抗性思想使得它在众多生成器模型中脱颖而出, 被广泛应用于计算机视觉 <span class="math inline">\((\mathrm{CV})\)</span> 、机器学习 <span class="math inline">\((\mathrm{ML})\)</span> 、语音处理 <span class="math inline">\((\mathrm{AS})\)</span> 等领域。在 arXiv 上以 generative adversarial networks generative adversarial nets 和 adversarial learning 为关键词的论文发文量总体逐年上升，并且 GAN 被应用在广阔的学科领域。这些数据说明了对 GAN 这一模型的研究的火热程度，也说明了该方法在人工智能等领域的重要性。</p>
<h1 id="原始生成对抗网络">原始生成对抗网络</h1>
<p>应用于图像生成的 GAN 的训练方法可以概述为“输入为一个随机向量 <span class="math inline">\(z\)</span>，生成器 <span class="math inline">\(G\)</span> 输出一幅图像 <span class="math inline">\(G(z)\)</span>，而判别器 <span class="math inline">\(D\)</span> 需要将真实图像 <span class="math inline">\(x\)</span> 与合成图像 <span class="math inline">\(G(z)\)</span> 区分开来。”</p>
<p>以下部分分别从随机变量的生成，TODO，等 TODO 个角度解读原始的 GAN 模型。</p>
<h2 id="随机变量的生成">随机变量的生成</h2>
<p>基于函数逆变换的方法已经可以通过简单的均匀分布随机变量生成符合特定分布的复杂随机变量，而这一方法与 GAN 模型有着深刻的联系。</p>
<p>囿于计算机的计算确定性，生成真正随机的数字在理论上是不行的。但是，计算机能够使用伪随机数生成器生成大致遵循 0 和 1 之间的均匀随机分布的数字序列。通过特定数学方法，可以定义生成某些特定数字序列的算法，这些数字的分布非常接近理论随机数的分布。</p>
<p>假设 X 是某一复杂随机变量，而 U 是 [0,1] 上的均匀随机变量。随机变量完全由其<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Cumulative_distribution_function">累积分布函数( CDF )</a>定义。随机变量的 CDF 是从随机变量的定义域到区间 [0,1] 的函数，并且在一维中定义为: <span class="math display">\[
C D F_{X}(x)=\mathbb{P}(X \leq x) \quad \in[0,1]
\]</span> 对于随机变量 U，我们有： <span class="math display">\[
C D F_{U}(u)=\mathbb{P}(U \leq u)=u \quad \forall u \in[0,1]
\]</span> 为简单起见，我们在这里假设函数 <span class="math inline">\(CDF_X\)</span> 是可逆的，它的逆表示为 <span class="math inline">\(CDF_X^{-1}\)</span>（通过使用函数的广义逆，该方法可以很容易地扩展到不可逆的情况，但这并非需要关注的重点），接着定义： <span class="math display">\[
Y=C D F_{X}^{-1}(U)
\]</span> 我们有： <span class="math display">\[
C D F_{Y}(y)=\mathbb{P}(Y \leq y)=\mathbb{P}\left(C D F_{X}^{-1}(U) \leq y\right)=\mathbb{P}\left(U \leq C D F_{X}(y)\right)=C D F_{X}(y)
\]</span> Y 和 X 具有相同的 CDF，从而定义了相同的随机变量。因此，通过如上定义 Y（作为均匀随机变量的函数），成功定义了具有目标分布的随机变量。</p>
<p>综上所述，逆变换方法是一种通过使均匀随机变量经过精心设计的<strong>变换函数（逆 CDF）</strong>来生成遵循给定分布的随机变量的方法。事实上，这个<strong>逆变换方法</strong>的概念可以扩展为具有一般性的<strong>变换方法</strong>——目标随机变量 $ X$ 作为某些更简单的随机变量 <span class="math inline">\(Y\)</span> 的函数， 而 <span class="math inline">\(Y\)</span> 不一定是均匀的，然后变换函数也不一定是逆 CDF。从概念上讲，<strong>变换函数</strong>的目的是对初始概率分布进行重塑。</p>
<div data-align="center">
<p><img width="1000" src="https://miro.medium.com/max/1400/1*Xoz06MKgbw7CZ8aNbMCt6A.jpeg"/></p>
</div>
<div data-align="center">
逆变换方法图示。蓝色：[0,1] 上的均匀分布。橙色：标准高斯分布。灰色：从均匀分布到高斯分布（逆 CDF）的映射。
</div>
<h2 id="生成模型">生成模型</h2>
<p>假设我们有大小为 <span class="math inline">\(n\times n\)</span> 像素的狗的黑白方形图像。我们可以将每个数据重塑为 <span class="math inline">\(N = n \times n\)</span> 维向量（通过将列堆叠在一起），这样狗的图像就可以用向量表示。然而，这并不意味着所有的向量都代表一只狗。生成狗的新图像的问题等价于在 <span class="math inline">\(N\)</span> 维向量空间上按照狗的概率分布生成新向量的问题。事实上，我们面临着一个针对特定概率分布生成随机变量的问题。</p>
<p>在这一点上，我们可以提到两件重要的事情。首先，“狗的概率分布”是一个非常复杂的分布在非常大的空间上的分布。其次，即使我们可以假设存在这种潜在分布，我们显然不知道如何明确地表达这种分布。前面的两点都使得从这个分布生成随机变量的过程非常困难。接下来让我们尝试解决这两个问题。</p>
<p>在大多数情况下，非常复杂的函数自然意味着神经网络建模。通过一个神经网络对变换函数进行建模，该神经网络将一个简单的 <span class="math inline">\(N\)</span> 维均匀随机变量作为输入，并返回另一个 <span class="math inline">\(N\)</span> 维随机变量作为输出，该变量在训练后应该遵循正确的“狗的概率分布”。</p>
<p>我们需要训练网络来表达正确的变换函数。为此，提出两种不同的训练方法。直接训练方法包括比较真实和生成的概率分布，并通过网络反向传播差异（误差）。这是规则生成匹配网络（GMN）的想法。对于间接训练方法，我们不直接比较真实分布和生成分布。相反，我们通过使这两个分布通过选择的下游任务来训练生成网络，这样生成网络相对于下游任务的优化过程将强制生成的分布接近真实分布。后一个想法即是 GAN 的基本思想。</p>
<p>GAN 的下游任务是真实样本和生成样本之间的区分任务。如上文所述，在 GAN 架构由两部分组成，首先是经过训练以尽可能地欺骗鉴别器的生成器，而后是鉴别器，它对真实和生成的数据进行采样，并尝试尽可能好地对它们进行分类。</p>
<p>具体而言，生成器是一个对变换函数进行建模的神经网络。它将一个简单的随机变量作为输入，并且必须在经过训练后返回一个遵循目标分布的随机变量。由于它非常复杂且未知，我们决定用另一个神经网络对鉴别器进行建模，该神经网络构建了一个判别函数。它将一个点（在我们的狗示例中为 N 维向量）作为输入，并以该点为真的概率作为输出。</p>
<p>一旦定义好，两个网络就可以以相反的目标联合训练：</p>
<ul>
<li>生成器的目标是欺骗判别器，因此训练生成神经网络以最大化真实数据和生成数据之间的最终分类误差</li>
<li>判别器的目标是检测虚假生成数据，因此训练判别神经网络以最小化最终分类误差</li>
</ul>
<p>因此，在训练过程的每次迭代中，生成网络的权重都会更新以增加分类误差，而判别网络的权重会更新以减少该误差。</p>
<p>相反的目标和两个网络的对抗性训练解释了对抗网络的名称：两个网络都试图互相击败。并且在这一过程中，它们都变得越来越好。从博弈论的角度来看，我们可以将此设置视为一个极小极大的两人游戏，他们之间的竞争使这两个网络在各自的目标上进步。</p>
<h2 id="数学原理">数学原理</h2>
<p>GAN 由两个网络组成：</p>
<ul>
<li>一个生成网络 <span class="math inline">\(G(.)\)</span>，它接受一个密度为 <span class="math inline">\(p_z\)</span> 的随机输入 <span class="math inline">\(z\)</span> 并返回一个输出 <span class="math inline">\(x_g = G(z)\)</span>，它应该遵循目标的概率分布</li>
<li>一个判别网络 <span class="math inline">\(D(.)\)</span>，它接受一个输入 <span class="math inline">\(x\)</span>，该输入 <span class="math inline">\(x\)</span> 可以是真实的数据 <span class="math inline">\(x_t\)</span>，其密度表示为 <span class="math inline">\(p_t\)</span>，或生成的数据 <span class="math inline">\(x_g\)</span>，其密度 <span class="math inline">\(p_g\)</span> 是由密度 <span class="math inline">\(p_z\)</span> 生成；它的返回值 <span class="math inline">\(D(x)\)</span> 为 x 是真实数据的概率</li>
</ul>
<p>如果我们以相同的比例向鉴别器输入真实和生成的数据，则鉴别器的预期绝对误差可以表示为： <span class="math display">\[
\begin{aligned}
E(G, D) &amp;=\frac{1}{2} \mathbb{E}_{x \sim p_{t}}[1-D(x)]+\frac{1}{2} \mathbb{E}_{z \sim p_{z}}[D(G(z))] \\
&amp;=\frac{1}{2}\left(\mathbb{E}_{x \sim p_{t}}[1-D(x)]+\mathbb{E}_{x \sim p_{g}}[D(x)]\right)
\end{aligned}
\]</span> 生成器的目标是欺骗鉴别器，鉴别器的目标是能够区分真实数据和生成的数据。因此，在训练生成器时，我们希望最大化这个误差，同时我们试图最小化判别器的误差： <span class="math display">\[
\max _{G}\left(\min _{D} E(G, D)\right)
\]</span> 对于任何给定的生成器 <span class="math inline">\(G\)</span>（以及生成的概率密度 <span class="math inline">\(p_g\)</span>），最好的鉴别器是能够最小化下式的鉴别器： <span class="math display">\[
\mathbb{E}_{x \sim p_{t}}[1-D(x)]+\mathbb{E}_{x \sim p_{g}}[D(x)]=\int_{\mathbb{R}}(1-D(x)) p_{t}(x)+D(x) p_{g}(x) d x
\]</span></p>
<h1 id="cgan">CGAN</h1>
<p>原始 GAN 对于生成器几乎没有任何约束，使得生成过程过于自由，模型变得难以控制 。 CGAN（conditional GAN）在原始GAN 的基础上增加了约束条件，控制了 GAN 过于自由的问题，使网络朝着既定的方向生成样本。</p>
<p>CGAN 的网络结构如下图所示，CGAN 生成器和判别器的输入多了一个约束项 <span class="math inline">\(y\)</span>，约束项 <span class="math inline">\(y\)</span> 可以是一个图像的类别标签，也可以是图像的部分属性数据。</p>
<div data-align="center">
<p><img width="1000" src="https://miro.medium.com/max/1396/0*L8loWBQIJoUrPR00.png"/></p>
</div>
<div data-align="center">
Conditional GAN 示意图
</div>
<h1 id="pix2pix">Pix2Pix</h1>
<p>Pix2Pix 模型是 CGAN 应用于 image translation 的开山之作，其目标是从一张图像转换为另一张图像，也即学习从输入图像到输出图像的映射。</p>
<p>该方法由 <a target="_blank" rel="noopener" href="http://web.mit.edu/phillipi/">Phillip Isola</a> 等人在他们 2016 年题为 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.07004">Image-to-Image Translation with Conditional Adversarial Networks</a> 的论文中提出，<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8100115">并于 2017 年在 CVPR 上发表</a>。</p>
<p>Pix2Pix 是 CGAN 的改进模型，在传统 GAN 和 CGAN 的基础上，网络架构有如下创新：</p>
<h2 id="u-net-生成器">U-Net 生成器</h2>
<p>传统生成器是一个编码器-解码器网络（E-D 网络）——首先是一系列下采样层，然后是瓶颈层，然后是一系列上采样层。</p>
<p>在 Pix2Pix 中，作者使用带有跳跃连接的 U-Net 架构作为 E-D 网络。</p>
<div data-align="center">
<p><img width="1000" src="https://i0.wp.com/neptune.ai/wp-content/uploads/Pix2pix-Key-Model-Architecture-Decisions_8.png?resize=840%2C556&ssl=1"/></p>
</div>
<div data-align="center">
U-Net Generator
</div>
<p>UNet由两个主要部分组成：</p>
<ol type="1">
<li>由卷积层（左侧）组成的收缩路径，在提取信息的同时对数据进行下采样<strong>。</strong></li>
<li>由向上转置卷积层（右侧）组成 的<strong>扩展路径，对信息进行上采样。</strong></li>
</ol>
<p>假设我们的下采样具有三个卷积层 <span class="math inline">\(C_l(1,2,3)\)</span>，那么我们必须确保我们的上采样具有三个转置卷积层 <span class="math inline">\(C_u(1,2,3)\)</span>。这是因为我们想使用<strong>跳过连接</strong>来连接相同大小的相应块。</p>
<div data-align="center">
<p><img width="1000" src="https://i0.wp.com/neptune.ai/wp-content/uploads/Pix2pix-Key-Model-Architecture-Decisions_3.png?resize=378%2C308&ssl=1"/></p>
</div>
<div data-align="center">
跳过连接网络示意图
</div>
<p><strong>下采样</strong></p>
<p>在下采样期间，每个卷积块提取空间信息并将信息传递给下一个卷积块以提取更多信息，直到它到达称为<strong>瓶颈</strong>的中间部分。上采样从瓶颈开始。</p>
<p><strong>上采样</strong></p>
<p>在上采样期间，每个转置卷积块扩展来自前一个块的信息，同时连接来自相应下采样块的信息。通过连接信息，网络可以学习根据这些信息组装更精确的输出。</p>
<p>该架构能够定位，也即能够逐像素地找到感兴趣的对象。此外，U-Net 还允许网络将上下文信息从较低分辨率的层传播到较高分辨率的层，使得网络生成高分辨率的样本。</p>
<h2 id="patchgan">patchGAN</h2>
<p>传统的直接机器学习采用损失函数回传梯度的方式进行参数优化，而采用的损失函数包括 L1 与 L2 两种： <span class="math display">\[
\text { L1 Loss Function }=\sum_{i=1}^{n}\left|y_{\text {true }}-y_{\text {predicted }}\right|
\]</span></p>
<p><span class="math display">\[
\text { L2 Loss Function }=\sum_{i=1}^{n}\left(y_{\text {true }}-y_{\text {predicted }}\right)^{2}
\]</span></p>
<p>对于 <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Autoencoder">AE</a> 和 <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Variational_autoencoder">VAE</a> 的相关研究表明，用 L1 loss 和 L2 loss 进行图像重建会导致结果较为模糊，L1 和 L2 loss 并不能很好的恢复图像的高频部分(图像中的边缘等)，但能较好地恢复图像的低频部分(图像中的色块)。为了能更好得对图像的局部做判断，Pix2Pix 中提出 patchGAN 的结构，其思想是将图像等分成若干个 patch，分别判断每个 Patch 的真假，最后再取平均。</p>
<p>具体而言，patchGAN 包含许多转置卷积块。它对图片 <span class="math inline">\(N\times N\)</span> 大小的部分做卷积。<span class="math inline">\(N\)</span> 为任意大小，它可以比原始图像小，但仍然能够产生高质量的结果。PatchGAN 可以有效地将图像建模为马尔可夫随机场，其中 <span class="math inline">\(N\times N\)</span> 被视为一个独立的 patch。因此，PatchGAN 可以理解为纹理损失的一种形式。由于鉴别器相比生成器具有更少的参数，因此鉴别器实际上运行更快。</p>
<h2 id="目标函数">目标函数</h2>
<p>按照 CGAN 基本原理一节的内容，传统目标函数为： <span class="math display">\[
L_{c G A N}(G, D)=\mathbb{E}_{x, y}[\log D(x, y)]+\mathbb{E}_{x, z}[\log (1-D(x, G(x, z)))]
\]</span> 同时定义与 ground truth 的 L1 loss： <span class="math display">\[
L_{L 1}(G)=\mathbb{E}_{x, y, z}\left[\|y-G(x, z)\|_{1}\right]
\]</span> 所以最终的目标函数为： <span class="math display">\[
G^{*}=\arg \min _{G} \max _{D} L_{c G A N}(G, D)+\lambda L_{L 1}(G)
\]</span> GAN 本身其实是一种相对于 L1 loss 更好的判别准则或者 loss。有时候单独使用 GAN loss 效果好，有时候与 L1 loss 配合起来效果好。在 pix2pix 中，作者将 L1 loss 和 GAN 相结合使用，因为作者认为 L1 loss 可以恢复图像的低频部分，而GAN 可以恢复图像的高频部分。</p>
<h1 id="gau-gan">Gau GAN</h1>
<p>从2017 年开始， Nvidia 不断提出了 <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/pix2pixHD">Pix2PixHD</a> 和 <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/vid2vid">Vid2Vid</a> 等方法应用于语义分割图重建为图像，2019 年的 <a target="_blank" rel="noopener" href="https://nvlabs.github.io/SPADE/">Semantic Image Synthesis with Spatially-Adaptive Normalization</a> 方法又在此领域有了显著进步。</p>
<p>不同于过去已有的图像合成GAN，简单地将卷积层、归一化层、以及非线性层堆叠在一起构成生成模型，GauGAN 引入了一个<strong>新的归一化层</strong>，在已有的 CGAN 基础上，GauGAN 引入了 SPADE 方法进行归一化。</p>
<p>作者认为，假设语义图输入单个类别，则一整张图都是同一个数，在经过 InstanceNorm 之后会输出某个固定的 bias，语义信息完全丢失。所以 pix2pix 就无法生成有效的图片，没有任何类别信息。针对这个问题，作者提出了<strong>SPADE（SPatially-Adaptive (DE)normalization）</strong>方法，通过 SPADE 使用语义图来调整 normalization 输出的结果，使其更好的具有语义信息，并将语义信息贯穿整个网络。并且其方法可以应对各种使用语义图的生成任务。</p>
<h2 id="无条件归一化">无条件归一化</h2>
<div data-align="center">
<p><img width="1000" src="https://lh3.googleusercontent.com/JGY13318_vHN7o1AsmiyU5cJcKi93Nk1JmeykzCCE5wkzP_oeqxqMN0UH65yjUR25IumSZhbexJM4fFEEpDLDah23pWRwOUDXct6rD0u3qM14orw-bAGnhe-CG8s6CYUiiv8CU08"/></p>
</div>
<div data-align="center">
In batch normalization, the statistics are computed over feature maps across all batches. In instance normalization, the statistics are computed over feature maps across a single image.
</div>
<p>传统的无条件归一化方法包括 batch normalization 与 instance normalization 等等。以 batch normalization(BN) 为例，其意义如下图所示：</p>
<div data-align="center">
<p><img width="1000" src="https://miro.medium.com/max/1400/1*tcvRJN-OadhUyps6HSO0og.jpeg"/></p>
</div>
<div data-align="center">
没有批量归一化(BN)的多层感知器(MLP)
</div>
<div data-align="center">
<p><img width="1000" src="https://miro.medium.com/max/1400/1*QcSkw489NgtpaMuwDhehaQ.jpeg"/></p>
</div>
<div data-align="center">
经过批量归一化(BN)的多层感知器(MLP)
</div>
<p>深度学习中的 normalization 通常包括三个步骤：</p>
<ol type="1">
<li>计算相关统计数据（如均值和标准差）</li>
<li>通过减去平均值并将这个数字除以标准偏差来标准化输入</li>
<li>重新缩放输入 <span class="math inline">\(y=γX+β\)</span>，其中参数 <span class="math inline">\(γ,β\)</span> 可学习</li>
</ol>
<p>具体到 Batch Normalization，在每个隐藏层，Batch Normalization 将信号转换如下： <span class="math display">\[
\mu=\frac{1}{n} \sum_{i} Z^{(i)}\\\sigma^{2}=\frac{1}{n} \sum_{i}\left(Z^{(i)}-\mu\right)^{2}\\Z_{\text {norm }}^{(i)}=\frac{Z^{(i)}-\mu}{\sqrt{\sigma^{2}-\epsilon}}\\\breve{Z}=\gamma * Z_{\text {norm }}^{(i)}+\beta
\]</span> BN 层首先确定整个批次的激活值的均值 <span class="math inline">\(𝜇\)</span> 和方差 <span class="math inline">\(σ^2\)</span>。然后它用对激活向量 <span class="math inline">\(Z^{(i)}\)</span> 进行归一化。 这样，每个神经元的输出在批次中遵循标准正态分布。（<span class="math inline">\(\epsilon\)</span> 是用于数值稳定性的常数）</p>
<div data-align="center">
<p><img width="1000" src="https://miro.medium.com/max/1400/1*TrjyZmHj_wInh6kFARuLZw.jpeg"/></p>
</div>
<div data-align="center">
以 3 个神经元的隐藏层与 batch size 为 3 来示意 batch normalization 的第一步。
</div>
<p>最终， BN 通过使用 <span class="math inline">\(𝛾\)</span> 和 <span class="math inline">\(𝛽\)</span> 这两个可训练参数应用线性变换来计算层的输出 <span class="math inline">\(Ẑ^{(i )}\)</span>。这样的步骤允许模型通过调整这两个参数为每个隐藏层选择最佳分布：</p>
<p>可以将 BN 层视为 normalize 部分跟 denormalize 部分。normalize 部分就是 BN 的归一化部分，计算 N 个featuremap某个通道的均值跟方差，用均值跟方差进行归一化：</p>
<ul>
<li><span class="math inline">\(𝛾\)</span> 允许调整标准偏差。</li>
<li><span class="math inline">\(𝛽\)</span> 允许调整偏差，在右侧或左侧移动曲线。</li>
</ul>
<div data-align="center">
<p><img width="1000" src="https://miro.medium.com/max/1400/1*zEkSLa9rpfEmINn5DmJbOA.jpeg"/></p>
</div>
<div data-align="center">
修改 𝛾 与 𝛽
</div>
<p>BN 取得的效果如下图所示：</p>
<div data-align="center">
<p><img width="1000" src="https://miro.medium.com/max/1316/1*gfzbC-GrOoVUfk1yU4DdfQ.png"/></p>
</div>
<div data-align="center">
在 ImageNet(2012) 上验证 BN 对训练的影响。 比较了五个网络：Inception 是普通的 Inception 网络，BN-X 是具有 BN 层的 Inception 网络。x1、x5、x30 是 3 种不同的学习率。BN-X-Sigmoid 是一个带有 BN 层的 Inception 网络，所有 ReLU 都被 sigmoid 取代。
</div>
<p>BN 层<strong>使训练更快</strong>，并<strong>允许更广泛的学习率</strong>，而不会影响训练收敛。</p>
<h2 id="spade">SPADE</h2>
<p>作者认为无条件归一化会导致语义信息的丢失。传统的 batch normalization 为每个通道学习其参数集，也即 <span class="math inline">\(𝛾\)</span> 和 <span class="math inline">\(𝛽\)</span> 是大小较小的向量或者就是一个数值，然而，在 SPADE 中，作者将 batch norm 的参数张量大小增加到与像素大小相同，也即 <span class="math inline">\(𝛾\)</span> 和 <span class="math inline">\(𝛽\)</span> 是与像素大小相同的张量，从而 SPADE 能为特征图中的每个像素学习其参数集。在原文中，<span class="math inline">\(𝛾\)</span> 和 <span class="math inline">\(𝛽\)</span> 是由分割 mask 通过卷积得到的，对语义分割 mask 来说，每个像素的值即是该像素点的类别。</p>
<h2 id="spade-generator">SPADE Generator</h2>
<p>作者结合残差卷积网络与 SPADE 构建了 SPADE Generator，它产生两个特征图：一个对应于逐像素的 <span class="math inline">\(𝛾\)</span>，另一个对应逐像素的 <span class="math inline">\(𝛽\)</span> ，两个特征图中的每个值代表用于重新缩放特征图中相应像素的值的参数。</p>
<div data-align="center">
<p><img width="1000" src="https://lh4.googleusercontent.com/uTrIfbzvp3B_hKcHFs4sfCFYujhukcDN1pVWS2iFdsXvpfZuyQFBa2EXXn5jNbadX4kWUdepyOMXoCJEjJnNJxdGs7f4Jo0szYOinakgvL6KHBc5FoF7Mae5gwyyNENWlKq6Je4r"/></p>
</div>
<div data-align="center">

</div>
<p>上图通过 Batch Norm( Sync Batch Norm ) 模块来进行统计计算，这一部分的设计与 Batch Normaliazation 完全相同。Sync Batch Norm 是在多 GPU 系统上进行的优化。一般来说，如果你有一个批量大小，比如 32 并且你有 8 个 GPU，那么 Jittor nn.BatchNorm2d layer 将在每个 GPU 上分别计算 4 个批次的统计数据并更新参数。在 Sync Batch Norm 中，统计数据是在整个 32 个图像上计算的。当每个 GPU 批量大小较小（例如 1 或 2）时，计算小批量的统计数据可能会产生非常嘈杂的估计，从而导致训练波动。因而此时同步归一化作用显著。</p>
<p>从 SPADE 模块中获得的特征图逐元素相乘并添加到归一化的输入图中。其中每个卷积层都遵循限制卷积滤波器的 Lipschitz 常数的 Spectral Norm。</p>
<blockquote>
<p><span class="math display">\[
\gamma_{c, y, x}^{i}(\mathbf{m}) \frac{h_{n, c, y, x}^{i}-\mu_{c}^{i}}{\sigma_{c}^{i}}+\beta_{c, y, x}^{i}(\mathbf{m})
\]</span></p>
<p>where <span class="math inline">\(h_{n, c, y, x}^{i}\)</span> is the activation at the site before normalization, <span class="math inline">\(\mu_{c}^{i}\)</span> and <span class="math inline">\(\sigma_{c}^{i}\)</span> are the mean and standard deviation of the activation in channel <span class="math inline">\(c\)</span> : <span class="math display">\[
\begin{aligned}
\mu_{c}^{i} &amp;=\frac{1}{N H^{i} W^{i}} \sum_{n, y, x} h_{n, c, y, x}^{i} \\
\sigma_{c}^{i} &amp;=\sqrt{\frac{1}{N H^{i} W^{i}} \sum_{n, y, x}\left(h_{n, c, y, x}^{i}\right)^{2}-\left(\mu_{c}^{i}\right)^{2} .}
\end{aligned}
\]</span> The variables <span class="math inline">\(\gamma_{c, y, x}^{i}(\mathbf{m})\)</span> and <span class="math inline">\(\beta_{c, y, x}^{i}(\mathbf{m})\)</span> in (1) are the learned modulation parameters of the normalization layer.</p>
</blockquote>
<h2 id="spade-方法的有效性">SPADE 方法的有效性</h2>
<p>首先，GauGAN 的输入是一个语义分割图，它被进一步独热编码。</p>
<p>这意味着 GAN 必须输入标签值相同的区域，并生成具有不同值的像素，以使它们看起来像一个真实的图像。为每个像素设置一组不同的 Batch Norm 参数有助于解决此任务，而不是为特征图中的每个通道提供一个 Batch Norm 参数。</p>
<p>另外，作者认为 SPADE 会导致更具<strong>辨别力</strong>的语义信息。相同的像素类别具有相同的分割结果与近似的卷积结果，因此 <span class="math inline">\(𝛾\)</span> 和 <span class="math inline">\(𝛽\)</span> 具有空间性，且这一空间性与分割 mask 保持一致，所以说 SPADE 是 SPatially-Adaptive 的，也即具有空间自适应性。</p>
<p>作者对 SPADE 的 SPatially-Adaptive 做了验证，考虑极端情况，当分割mask只有一种类别的时候，如下图的天空和草地，在经过 Instance normalization 之后，pix2pixHD 对这两种分割 mask 的输出都一样，SPADE 能对其进行区分。</p>
<div data-align="center">
<p><img width="1000" src="https://lh3.googleusercontent.com/EsG7dU7Tw25uU8M1e-i95zeU5OTnDxHf4nCT7JNkWdOw_BqGrq20J1IQ3u8mLy4TVHMkHBPOjuKXlFrZOvtcPO9zcvOfP9BbMF0tdStzO8y7NLQqVymdaloCxOWoO_xrMqaSNz84"/></p>
</div>
<div data-align="center">

</div>
<p>而后，作者探讨了 SPADE 和 Instance Norm 的输出在给定相同标签的语义图的情况下有何不同。</p>
<p>作者认为语义信息在 normailization 过程中被大量删除。SPADE 和 Instance Norm 中的标准化步骤是相同的，它们不同的地方是重新缩放步骤。具体而言，在 Pix2Pix 中，Instance Norm 层的参数是不可学习的，Instance Norm 只是进行归一化（<span class="math inline">\(γ\)</span> 设置为 1 和 <span class="math inline">\(β\)</span> 设置为 0）。然而，在 GauGAN 中，SPADE 具有可学习的参数。最后，Instance Norm 使用的 batch size 为 1，而 SPADE 和 Batch Norm 都可以利用更大的批量大小，可以有效减少统计噪声。</p>
<h2 id="鉴别器与编码器">鉴别器与编码器</h2>
<p>Gau GAN 采用了 PatchGAN 架构作为鉴别器，已在上文叙述。</p>
<div data-align="center">
<p><img width="1000" src="https://lh6.googleusercontent.com/YA9bZD9o87cRTpEsIjba0tuCCfm3anbZAoMZZpoHpzpP-aaoYeCm4VyZeucswmlENvFFjmcdozCPeAhe_8g-0eP0SOJaEiXvH7tm_MIhSTv_ujMcwXbb14NdZAeLmmPR8j8EF3ra"/></p>
</div>
<div data-align="center">

</div>
<p>在编码器部分，与普通 GAN 不同，GauGAN 不采用随机噪声向量，而仅采用语义图。这意味着给定单个输入语义图，输出将始终是确定性的。这违背了图像合成的精神，因为 GauGAN 对同一语义分割图输出重建不同图像的能力受到高度重视。</p>
<p>为此，作者设计了一个编码器。编码器取出一张图像的语义分割图，将其编码成两个向量。这两个向量用作正态高斯分布的均值和标准差。然后从这个分布中采样一个随机向量，与输入语义图一起连接作为生成器的输入。</p>
<p>同时，编码器输入的随机向量还可以作为生成图像的风格信息，每个随机向量将生成具有相同语义布局但不同模态特征（如颜色、亮度等）的图像。</p>
<div data-align="center">
<p><img width="1000" src="https://lh3.googleusercontent.com/oODlvkIcHeV4-zhgN0HoL-VupxZHvL7krSt_TMMnKU-TdBvTYq6BO3A5ChngwOdpuYE1U639In-wMX-YRmFK0GFz5yX1zcVoCRRlcqtThnlQBtR-Ot8CFhEcFZTW_GwnXeXlp158"/></p>
</div>
<div data-align="center">

</div>
<h2 id="损失函数">损失函数</h2>
<p><strong>Multiscale Adversarial Loss</strong></p>
<p>GauGAN 包含铰链损失，这在 SAGAN 和 Geometric GAN 等论文中也有所体现。下面是损失函数： <span class="math display">\[
\begin{array}{l}
L_{D}=-\mathbb{E}_{(x, y) \sim p_{\text {data }}}[\min (0,-1+D(x, y))]-\mathbb{E}_{z \sim p_{z}, y \sim p_{\text {data }}}[\min (0,-1-D(G(z), y))] \\
L_{G}=-\mathbb{E}_{z \sim p_{z}, y \sim p_{\text {data }}} D(G(z), y)
\end{array}
\]</span> 给定生成器生成的图像，我们创建一个 image pyramid，将生成的图像调整为多个比例。然后，使用判别器计算每个尺度的真实度分数并反向传播损失。</p>
<p><strong>Feature Matching Loss</strong></p>
<p>特征匹配损失鼓励 GAN 不仅能生成欺骗生成器的图像，而且生成的图像还应该具有与真实图像相同的统计特性。为了做到这一点，GauGAN 加入了真实图像与生成图像之间的 L1 距离。</p>
<p>如下所示，为生成图像的所有尺度计算特征匹配损失： <span class="math display">\[
L_{F M}\left(G, D_{k}\right)=\mathbb{E}_{s, x} \sum_{i=1}^{T} \frac{1}{N_{i}}\left[\left\|D_{k}^{(i)}(s, x)-D_{k}^{(i)}(s, G(s))\right\|_{1}\right]
\]</span> <strong>VGG Loss</strong></p>
<p>VGG Loss 类似于 Feature Matching Loss，唯一的区别是没有使用鉴别器来计算特征图，而是使用在 Imagenet 上预训练的 VGG-19 来计算真实图像和假图像的特征图。然后我们惩罚这些地图之间的 L1 距离。 <span class="math display">\[
L_{V G G}\left(G, D_{k}\right)=\underset{s, x}{\mathbb{E}} \sum_{i=1}^{5} \frac{1}{2^{i}}\left[\left\|V G G\left(x, M_{i}\right)-V G G\left(G(s), M_{i}\right)\right\|_{1}\right]
\]</span> where <span class="math inline">\(V G G(x, m)\)</span> is the feature map m of VGG19 when <span class="math inline">\(x\)</span> is the input. and <span class="math inline">\(M=\{\)</span> "relu1_1 ", "relu2_1" "relu3_1 " "relu4_1 " "relu5_1" <span class="math inline">\(\}\)</span></p>
<p><strong>Encoder Loss</strong></p>
<p>作者对编码器使用 KL 散度损失： <span class="math display">\[
L_{K L D}=D_{k l}(q(z \mid x)|| p(z))
\]</span> 在上式中，<span class="math inline">\(q(z \mid X)\)</span> 称为变分分布，我们从中抽出随机向量 <span class="math inline">\(z\)</span> 并给定真实图像 <span class="math inline">\(X\)</span> ，而 <span class="math inline">\(p(z)\)</span> 是标准高斯分布。</p>
<p>上述损失函数可以理解为 Variational Auto-Encoder Loss 中的正则化损失项。使用编码器，GauGAN 起到了变分自动编码器中的解码器的作用，而 KL 散度损失项充当编码器的正则化项。这种损失会惩罚编码器预测的分布与零均值高斯分布之间的 KL 散度。如果缺失这种损失，编码器可以通过为数据集中的每个训练示例分配一个不同的随机向量来作弊，而不是实际学习一个捕获数据模态的分布。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Eren Zhao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://example.com/2022/06/22/Lecture/2022%20Spring/%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%A4%A7%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/">http://example.com/2022/06/22/Lecture/2022%20Spring/%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%A4%A7%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A7%91%E7%A0%94/">科研</a><a class="post-meta__tags" href="/tags/2022%E5%A4%8F%E5%AD%A3/">2022夏季</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-62b11572b25ab3ab" async="async"></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2022/06/22/%E9%9A%8F%E7%AC%94/%E5%BF%83%E5%BF%83%E5%BF%B5%E5%BF%B5/%E7%AD%94%E7%96%91%E5%9D%8A%E6%8A%80%E6%9C%AF%E7%BB%84%E6%94%B9%E9%9D%A9%E6%96%B9%E6%A1%88/"><img class="next-cover" src="https://pic.imgdb.cn/item/61eccb3f2ab3f51d91d60b7d.jpg" onerror="onerror=null;src='https://wkphoto.cdn.bcebos.com/1f178a82b9014a9058a735deb9773912b31bee3c.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">答疑坊小程序开发组改革</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2022/05/24/Lecture/2022%20Spring/Reinforcement/" title="重力四子棋"><img class="cover" src="https://pic.imgdb.cn/item/61eccc682ab3f51d91d712d5.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-24</div><div class="title">重力四子棋</div></div></a></div><div><a href="/2022/05/17/Lecture/2022%20Spring/binaryDivdence/" title="深度学习基础"><img class="cover" src="https://pic.imgdb.cn/item/6234754d5baa1a80ab39c4c4.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-17</div><div class="title">深度学习基础</div></div></a></div><div><a href="/2022/06/06/Lecture/2022%20Spring/deep_learning/" title="Segement Me If U Can"><img class="cover" src="https://pic.imgdb.cn/item/61f2b2892ab3f51d91051bb8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-06</div><div class="title">Segement Me If U Can</div></div></a></div><div><a href="/2022/06/04/%E7%A3%95%E7%9B%90/Gau%20GAN/" title="Understanding Gau GAN"><img class="cover" src="https://pic.imgdb.cn/item/61f0fc662ab3f51d9172fef0.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-04</div><div class="title">Understanding Gau GAN</div></div></a></div><div><a href="/2022/05/06/%E7%A3%95%E7%9B%90/Enhancing%20photorealism%20enhancement/" title="Enhancing photorealism enhancement"><img class="cover" src="https://pic.imgdb.cn/item/61ed142e2ab3f51d911d2e34.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-06</div><div class="title">Enhancing photorealism enhancement</div></div></a></div><div><a href="/2022/01/10/%E7%A3%95%E7%9B%90/VIT/" title="Vision Transformer Learning Log"><img class="cover" src="https://pic.imgdb.cn/item/61f0fc612ab3f51d9172f968.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-10</div><div class="title">Vision Transformer Learning Log</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div><div id="comment-switch"><span class="first-comment">Valine</span><span class="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpeg" onerror="this.onerror=null;this.src='/img/friend.jpg'" alt="avatar"/></div><div class="author-info__name">Eren Zhao</div><div class="author-info__description">BJ 的单身日记一周年纪念</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">179</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">56</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">12</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/zhaochenyang20"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">越南太乱，我只想去码头上整点薯条...</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8E%9F%E5%A7%8B%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C"><span class="toc-number">2.</span> <span class="toc-text">原始生成对抗网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E7%94%9F%E6%88%90"><span class="toc-number">2.1.</span> <span class="toc-text">随机变量的生成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.2.</span> <span class="toc-text">生成模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86"><span class="toc-number">2.3.</span> <span class="toc-text">数学原理</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#cgan"><span class="toc-number">3.</span> <span class="toc-text">CGAN</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#pix2pix"><span class="toc-number">4.</span> <span class="toc-text">Pix2Pix</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#u-net-%E7%94%9F%E6%88%90%E5%99%A8"><span class="toc-number">4.1.</span> <span class="toc-text">U-Net 生成器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#patchgan"><span class="toc-number">4.2.</span> <span class="toc-text">patchGAN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"><span class="toc-number">4.3.</span> <span class="toc-text">目标函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#gau-gan"><span class="toc-number">5.</span> <span class="toc-text">Gau GAN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A0%E6%9D%A1%E4%BB%B6%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-number">5.1.</span> <span class="toc-text">无条件归一化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#spade"><span class="toc-number">5.2.</span> <span class="toc-text">SPADE</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#spade-generator"><span class="toc-number">5.3.</span> <span class="toc-text">SPADE Generator</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#spade-%E6%96%B9%E6%B3%95%E7%9A%84%E6%9C%89%E6%95%88%E6%80%A7"><span class="toc-number">5.4.</span> <span class="toc-text">SPADE 方法的有效性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%89%B4%E5%88%AB%E5%99%A8%E4%B8%8E%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">5.5.</span> <span class="toc-text">鉴别器与编码器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">5.6.</span> <span class="toc-text">损失函数</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/06/22/Lecture/2022%20Spring/%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%A4%A7%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/" title="图形学开源报告"><img src="https://pic.imgdb.cn/item/61eccb5a2ab3f51d91d621f8.jpg" onerror="this.onerror=null;this.src='https://wkphoto.cdn.bcebos.com/1f178a82b9014a9058a735deb9773912b31bee3c.jpg'" alt="图形学开源报告"/></a><div class="content"><a class="title" href="/2022/06/22/Lecture/2022%20Spring/%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%A4%A7%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/" title="图形学开源报告">图形学开源报告</a><time datetime="2022-06-22T11:40:15.804Z" title="Created 2022-06-22 19:40:15">2022-06-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/22/%E9%9A%8F%E7%AC%94/%E5%BF%83%E5%BF%83%E5%BF%B5%E5%BF%B5/%E7%AD%94%E7%96%91%E5%9D%8A%E6%8A%80%E6%9C%AF%E7%BB%84%E6%94%B9%E9%9D%A9%E6%96%B9%E6%A1%88/" title="答疑坊小程序开发组改革"><img src="https://pic.imgdb.cn/item/61eccb3f2ab3f51d91d60b7d.jpg" onerror="this.onerror=null;this.src='https://wkphoto.cdn.bcebos.com/1f178a82b9014a9058a735deb9773912b31bee3c.jpg'" alt="答疑坊小程序开发组改革"/></a><div class="content"><a class="title" href="/2022/06/22/%E9%9A%8F%E7%AC%94/%E5%BF%83%E5%BF%83%E5%BF%B5%E5%BF%B5/%E7%AD%94%E7%96%91%E5%9D%8A%E6%8A%80%E6%9C%AF%E7%BB%84%E6%94%B9%E9%9D%A9%E6%96%B9%E6%A1%88/" title="答疑坊小程序开发组改革">答疑坊小程序开发组改革</a><time datetime="2022-06-22T08:19:52.564Z" title="Created 2022-06-22 16:19:52">2022-06-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/16/Lecture/2022%20Spring/%E8%AF%B7%E4%BB%A5%E4%BD%A0%E7%9A%84%E5%90%8D%E5%AD%97%E5%91%BC%E5%94%A4%E6%88%91/" title="我们为什么会选择社会化抚养"><img src="https://s2.loli.net/2022/03/08/jPxmqXH5YvbsgOu.png" onerror="this.onerror=null;this.src='https://wkphoto.cdn.bcebos.com/1f178a82b9014a9058a735deb9773912b31bee3c.jpg'" alt="我们为什么会选择社会化抚养"/></a><div class="content"><a class="title" href="/2022/06/16/Lecture/2022%20Spring/%E8%AF%B7%E4%BB%A5%E4%BD%A0%E7%9A%84%E5%90%8D%E5%AD%97%E5%91%BC%E5%94%A4%E6%88%91/" title="我们为什么会选择社会化抚养">我们为什么会选择社会化抚养</a><time datetime="2022-06-16T11:50:59.953Z" title="Created 2022-06-16 19:50:59">2022-06-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/10/%E9%9A%8F%E7%AC%94/%E5%8D%9A%E6%96%87/singapore/" title="工作 3 年，我又从新加坡反向润回国了"><img src="https://pic.imgdb.cn/item/623476885baa1a80ab3c8b56.jpg" onerror="this.onerror=null;this.src='https://wkphoto.cdn.bcebos.com/1f178a82b9014a9058a735deb9773912b31bee3c.jpg'" alt="工作 3 年，我又从新加坡反向润回国了"/></a><div class="content"><a class="title" href="/2022/06/10/%E9%9A%8F%E7%AC%94/%E5%8D%9A%E6%96%87/singapore/" title="工作 3 年，我又从新加坡反向润回国了">工作 3 年，我又从新加坡反向润回国了</a><time datetime="2022-06-10T13:00:55.188Z" title="Created 2022-06-10 21:00:55">2022-06-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/06/Lecture/2022%20Spring/deep_learning/" title="Segement Me If U Can"><img src="https://pic.imgdb.cn/item/61f2b2892ab3f51d91051bb8.jpg" onerror="this.onerror=null;this.src='https://wkphoto.cdn.bcebos.com/1f178a82b9014a9058a735deb9773912b31bee3c.jpg'" alt="Segement Me If U Can"/></a><div class="content"><a class="title" href="/2022/06/06/Lecture/2022%20Spring/deep_learning/" title="Segement Me If U Can">Segement Me If U Can</a><time datetime="2022-06-06T08:50:44.589Z" title="Created 2022-06-06 16:50:44">2022-06-06</time></div></div></div></div></div></div></main><footer id="footer" style="background: ＃191970"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Eren Zhao</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div><script type="text/javascript" id="maid-script" src="https://unpkg.com/mermaid@undefined/dist/mermaid.min.js?v=undefined"></script><script>if (window.mermaid) {
  var options = JSON.parse(document.getElementById('maid-script').getAttribute('mermaidoptioins'));
  mermaid.initialize(options);
}</script></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="rightside.chat_btn"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Local search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script>function loadDisqus () {
  var disqus_config = function () {
    this.page.url = 'http://example.com/2022/06/22/Lecture/2022%20Spring/%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%A4%A7%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/'
    this.page.identifier = '2022/06/22/Lecture/2022 Spring/图形学大作业报告/'
    this.page.title = '图形学开源报告'
  };

  window.disqusReset = () => {
    DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  if (window.DISQUS) disqusReset()
  else {
    (function() { 
      var d = document, s = d.createElement('script');
      s.src = 'https://.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  }
}

if ('Valine' === 'Disqus' || !true) {
  if (true) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
  else loadDisqus()
} else {
  function loadOtherComment () {
    loadDisqus()
  }
}
</script></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/"});</script></body></html>